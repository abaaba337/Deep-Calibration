{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "import torch\n",
    "import numpy as np\n",
    "from src.Heston import HestonModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "def Savepickle(obj, doc_path):\n",
    "  with open(doc_path, 'wb') as file:\n",
    "         dill.dump(obj, file)     \n",
    "\n",
    "def Readpickle(doc_path):\n",
    "    with open(doc_path, 'rb') as file:\n",
    "        return dill.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# European"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point-wise Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Settings\n",
    "paras = {\n",
    "                'rf'    : (0, 0.06),\n",
    "                'v0'    : (1e-3, 0.15),\n",
    "                'vbar'  : (1e-3, 0.10),\n",
    "                'kappa' : (1e-3, 5.0),\n",
    "                'xi'    : (1e-3, 1.0),\n",
    "                'rho'   : (-1.0, 1.0),\n",
    "                'M'     : (0.50, 1.5),\n",
    "                'tau'   : (0.25, 2.0)\n",
    "            }\n",
    "\n",
    "para_index = ['rf', 'v0', 'vbar', 'kappa', 'xi', 'rho', 'M', 'tau']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling Process finished : 10000/70000\n",
      "Sampling Process finished : 20000/70000\n",
      "Sampling Process finished : 30000/70000\n",
      "Sampling Process finished : 40000/70000\n",
      "Sampling Process finished : 50000/70000\n",
      "Sampling Process finished : 60000/70000\n",
      "Sampling Process finished : 70000/70000\n",
      "3371\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic data\n",
    "x = [] # [ [rf, v0,  vbar, kappa, xi, rho, M, tau] ]\n",
    "y = []  # [ sigmaIV ]\n",
    "\n",
    "i , Nsample , failed_sample_num = 1 , 70000 , 0\n",
    "\n",
    "while i <= Nsample:\n",
    "    S0    = 1\n",
    "    rf    = np.random.uniform(paras['rf'][0],paras['rf'][1])\n",
    "    v0    = np.random.uniform(paras['v0'][0],paras['v0'][1])\n",
    "    vbar  = np.random.uniform(paras['vbar'][0],paras['vbar'][1])\n",
    "    kappa = np.random.uniform(paras['kappa'][0],paras['kappa'][1])\n",
    "    xi    = np.random.uniform(paras['xi'][0],paras['xi'][1])\n",
    "    rho   = np.random.uniform(paras['rho'][0],paras['rho'][1])\n",
    "    M     = np.random.uniform(paras['M'][0],paras['M'][1])\n",
    "    tau   = np.random.uniform(paras['tau'][0],paras['tau'][1])\n",
    "    \n",
    "    try :\n",
    "        model = HestonModel(rf, kappa, vbar, xi, rho)\n",
    "        price = model.PriceEuropean(S0, v0, tau, K=M*S0)\n",
    "        sigmaIV = model.getImpliedVol(price, S0, tau, K=M*S0)\n",
    "        \n",
    "        if sigmaIV == -1 :\n",
    "            failed_sample_num += 1 \n",
    "            continue\n",
    "\n",
    "        else :\n",
    "            \n",
    "            x.append([rf, v0,  vbar, kappa, xi, rho, M, tau])\n",
    "            y.append(sigmaIV)\n",
    "            \n",
    "            if i%10000 == 0:\n",
    "                print('Sampling Process finished : %d/%d'%(i, Nsample))   \n",
    "            i += 1\n",
    "        \n",
    "    except :\n",
    "        failed_sample_num += 1 \n",
    "        continue\n",
    "    \n",
    "print(failed_sample_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing x , y\n",
    "def transformX(x):\n",
    "    xx = np.array(x.copy())\n",
    "    for i in range(len(para_index)):\n",
    "        lb , ub = paras[para_index[i]]\n",
    "        xx[:,i] = ( 2*xx[:,i] - (ub+lb) ) / (ub-lb)\n",
    "    return xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To tensor and construct dataloader\n",
    "x , y = transformX(x) , np.array(y).reshape(-1, 1)\n",
    "x_tensor = torch.tensor(x, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "dataset = TensorDataset(x_tensor, y_tensor)\n",
    "\n",
    "# Dataset sizes\n",
    "total_size = len(dataset)\n",
    "train_size = int(total_size * 0.6)\n",
    "validate_size = int(total_size * 0.2)\n",
    "test_size  = total_size - train_size - validate_size\n",
    "\n",
    "# Construct datasets\n",
    "train, validate, test = random_split(dataset, [train_size, validate_size, test_size])\n",
    "\n",
    "# Save data\n",
    "data_saved = {\n",
    "    'paras'            : paras,          # paramter settings and range\n",
    "    'para_index'       : para_index,     # input order    \n",
    "    'Xnormalized_data' : dataset,        # dataset.tensors gives (x_tensor, y_tensor) \n",
    "    'train'            : train,          # train[:] gives (x_tensor, y_tensor) in train\n",
    "    'validate'         : validate,\n",
    "    'test'             : test      \n",
    "}\n",
    "\n",
    "# Savepickle(data_saved, doc_path='./data/European/point_wise_training_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid-base Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Parameter Settings\n",
    "paras = {\n",
    "                'rf'    : (0, 0.06),\n",
    "                'v0'    : (1e-3, 0.15),\n",
    "                'vbar'  : (1e-3, 0.10),\n",
    "                'kappa' : (1e-3, 5.0),\n",
    "                'xi'    : (1e-3, 1.0),\n",
    "                'rho'   : (-1.0, 1.0),\n",
    "                'M'     : np.arange(0.5,1.6,0.1),\n",
    "                'tau'   : np.arange(0.25,2.25,0.25)\n",
    "            }\n",
    "\n",
    "para_index = ['rf', 'v0', 'vbar', 'kappa', 'xi', 'rho']\n",
    "recover_y_dim = (len(paras['tau']) , len(paras['M']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "790\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic data\n",
    "x  = [] # [ [rf, v0,  vbar, kappa, xi, rho] ]\n",
    "y  = [] # [ sigmaIV ]\n",
    "\n",
    "k , Nsample , failed_sample_num , flag = 1 , 795 , 0 , False #17000\n",
    "M , tau = paras['M'] , paras['tau']\n",
    "M, tau = np.meshgrid(M, tau)\n",
    "\n",
    "\n",
    "while k <= Nsample:\n",
    "    rf    = np.random.uniform(paras['rf'][0],paras['rf'][1])\n",
    "    S0    = 1.0\n",
    "    v0    = np.random.uniform(paras['v0'][0],paras['v0'][1])\n",
    "    vbar  = np.random.uniform(paras['vbar'][0],paras['vbar'][1])\n",
    "    kappa = np.random.uniform(paras['kappa'][0],paras['kappa'][1])\n",
    "    xi    = np.random.uniform(paras['xi'][0],paras['xi'][1])\n",
    "    rho   = np.random.uniform(paras['rho'][0],paras['rho'][1])\n",
    "    \n",
    "    model = HestonModel(rf, kappa, vbar, xi, rho)\n",
    "    sigmaIV = np.empty_like(M)\n",
    "    \n",
    "    for i in range(M.shape[0]) :\n",
    "        for j in range(M.shape[1]) :\n",
    "            price = model.PriceEuropean(S0, v0, tau[i,j], M[i,j])\n",
    "            try :\n",
    "                sigmaIV[i,j] = model.getImpliedVol(price, S0, tau[i,j], M[i,j])\n",
    "            except :\n",
    "                failed_sample_num += 1 \n",
    "                flag = True\n",
    "                break\n",
    "        if flag :\n",
    "            break\n",
    "       \n",
    "    if flag :\n",
    "        flag = False\n",
    "        failed_sample_num += 1\n",
    "        continue\n",
    "    \n",
    "    else :\n",
    "        x.append([rf, v0,  vbar, kappa, xi, rho])\n",
    "        y.append(sigmaIV.flatten())\n",
    "        \n",
    "        if k%1000 == 0:\n",
    "            print('Sampling Process finished : %d/%d'%(k, Nsample))\n",
    "        k += 1\n",
    "\n",
    "print(failed_sample_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing x , y\n",
    "def transformX(x):\n",
    "    xx = np.array(x.copy())\n",
    "    for i in range(len(para_index)):\n",
    "        lb , ub = paras[para_index[i]]\n",
    "        xx[:,i] = ( 2*xx[:,i] - (ub+lb) ) / (ub-lb)\n",
    "    return xx\n",
    "        \n",
    "# def inverse_transformX(x):\n",
    "#     xx = x.copy()\n",
    "#     for i in range(len(para_index)):\n",
    "#         lb , ub = paras[para_index[i]]\n",
    "#         xx[:,i] = ( (ub-lb) * xx[:,i] + (ub+lb) ) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To tensor and construct dataloader\n",
    "x , y = transformX(x) , np.array(y)\n",
    "x_tensor = torch.tensor(x, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "dataset = TensorDataset(x_tensor, y_tensor)\n",
    "\n",
    "# Dataset sizes\n",
    "total_size = len(dataset)\n",
    "train_size = int(total_size * 0.6)\n",
    "validate_size = int(total_size * 0.2)\n",
    "test_size  = total_size - train_size - validate_size\n",
    "\n",
    "# Construct datasets\n",
    "train, validate, test = random_split(dataset, [train_size, validate_size, test_size])\n",
    "\n",
    "# Save data\n",
    "data_saved = {\n",
    "    'paras'            : paras,          # paramter settings and range\n",
    "    'para_index'       : para_index,     # input order  \n",
    "    'Xnormalized_data'  : dataset,        # dataset.tensors gives (x_tensor, y_tensor) \n",
    "    'train'            : train,          # train[:] gives (x_tensor, y_tensor) in train\n",
    "    'validate'         : validate,\n",
    "    'test'             : test,     \n",
    "    'recover_y_dim'    : recover_y_dim\n",
    "}\n",
    "\n",
    "Savepickle(data_saved, doc_path='./data/European/grid_based_training_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caliobration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Settings\n",
    "paras = {\n",
    "                'rf'    : (0, 0.06),\n",
    "                'v0'    : (1e-3, 0.15),\n",
    "                'vbar'  : (1e-3, 0.10),\n",
    "                'kappa' : (1e-3, 5.0),\n",
    "                'xi'    : (1e-3, 1.0),\n",
    "                'rho'   : (-1.0, 1.0),\n",
    "                'M'     : np.arange(0.5,1.6,0.1),\n",
    "                'tau'   : np.arange(0.25,2.25,0.25)\n",
    "            }\n",
    "\n",
    "para_index = ['rf', 'v0', 'vbar', 'kappa', 'xi', 'rho']\n",
    "recover_y_dim = (len(paras['tau']) , len(paras['M']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling Process finished : 1000/5000\n",
      "Sampling Process finished : 2000/5000\n",
      "Sampling Process finished : 3000/5000\n",
      "Sampling Process finished : 4000/5000\n",
      "Sampling Process finished : 5000/5000\n",
      "5186\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic data\n",
    "x  = [] # [ [rf, v0,  vbar, kappa, xi, rho] ]\n",
    "y  = [] # [ sigmaIV ]\n",
    "\n",
    "k , Nsample , failed_sample_num , flag = 1 , 5000 , 0 , False #17000\n",
    "M , tau = paras['M'] , paras['tau']\n",
    "M, tau = np.meshgrid(M, tau)\n",
    "\n",
    "\n",
    "while k <= Nsample:\n",
    "    rf    = 0.025\n",
    "    S0    = 1.0\n",
    "    v0    = np.random.uniform(paras['v0'][0],paras['v0'][1])\n",
    "    vbar  = np.random.uniform(paras['vbar'][0],paras['vbar'][1])\n",
    "    kappa = np.random.uniform(paras['kappa'][0],paras['kappa'][1])\n",
    "    xi    = np.random.uniform(paras['xi'][0],paras['xi'][1])\n",
    "    rho   = np.random.uniform(paras['rho'][0],paras['rho'][1])\n",
    "    \n",
    "    model = HestonModel(rf, kappa, vbar, xi, rho)\n",
    "    sigmaIV = np.empty_like(M)\n",
    "    \n",
    "    for i in range(M.shape[0]) :\n",
    "        for j in range(M.shape[1]) :\n",
    "            price = model.PriceEuropean(S0, v0, tau[i,j], M[i,j])\n",
    "            try :\n",
    "                sigmaIV[i,j] = model.getImpliedVol(price, S0, tau[i,j], M[i,j])\n",
    "            except :\n",
    "                failed_sample_num += 1 \n",
    "                flag = True\n",
    "                break\n",
    "        if flag :\n",
    "            break\n",
    "       \n",
    "    if flag :\n",
    "        flag = False\n",
    "        failed_sample_num += 1\n",
    "        continue\n",
    "    \n",
    "    else :\n",
    "        x.append([rf, v0,  vbar, kappa, xi, rho])\n",
    "        y.append(sigmaIV.flatten())\n",
    "        \n",
    "        if k%1000 == 0:\n",
    "            print('Sampling Process finished : %d/%d'%(k, Nsample))\n",
    "        k += 1\n",
    "\n",
    "print(failed_sample_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing x , y\n",
    "def transformX(x):\n",
    "    xx = np.array(x.copy())\n",
    "    for i in range(len(para_index)):\n",
    "        lb , ub = paras[para_index[i]]\n",
    "        xx[:,i] = ( 2*xx[:,i] - (ub+lb) ) / (ub-lb)\n",
    "    return xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To tensor and construct dataloader\n",
    "x , y = transformX(x) , np.array(y)\n",
    "x_tensor = torch.tensor(x, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "dataset = TensorDataset(x_tensor, y_tensor)\n",
    "\n",
    "# Save data\n",
    "data_saved = {\n",
    "    'paras'            : paras,          # paramter settings and range\n",
    "    'para_index'       : para_index,     # input order  \n",
    "    'Xnormalized_data'  : dataset,        # dataset.tensors gives (x_tensor, y_tensor)   \n",
    "    'recover_y_dim'    : recover_y_dim\n",
    "}\n",
    "\n",
    "Savepickle(data_saved, doc_path='./data/European/calibration_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Grid-based Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Settings\n",
    "paras = {\n",
    "                'rf'    : (0, 0.06),\n",
    "                'v0'    : (1e-3, 0.15),\n",
    "                'vbar'  : (1e-3, 0.10),\n",
    "                'kappa' : (1e-3, 5.0),\n",
    "                'xi'    : (1e-3, 1.0),\n",
    "                'rho'   : (-1.0, 1.0),\n",
    "                'M'     : (0.50, 1.5),\n",
    "                'tau'   : (0.25, 2.0)\n",
    "            }\n",
    "\n",
    "para_index = ['rf', 'v0', 'vbar', 'kappa', 'xi', 'rho']\n",
    "recover_y_dim = (8,11) # (len(paras['tau']) , len(paras['M']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling Process finished : 100/795\n",
      "Sampling Process finished : 200/795\n",
      "Sampling Process finished : 300/795\n",
      "Sampling Process finished : 400/795\n",
      "Sampling Process finished : 500/795\n",
      "Sampling Process finished : 600/795\n",
      "Sampling Process finished : 700/795\n",
      "618\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic data\n",
    "x  = [] # [ [rf, v0,  vbar, kappa, xi, rho] ]\n",
    "y  = [] # [ sigmaIV ]\n",
    "\n",
    "k , Nsample , failed_sample_num , flag = 1 , 795 , 0 , False #17000\n",
    "M , tau = paras['M'] , paras['tau']\n",
    "M, tau = np.meshgrid(M, tau)\n",
    "\n",
    "\n",
    "while k <= Nsample:\n",
    "    rf    = np.random.uniform(paras['rf'][0],paras['rf'][1])\n",
    "    S0    = 1.0\n",
    "    v0    = np.random.uniform(paras['v0'][0],paras['v0'][1])\n",
    "    vbar  = np.random.uniform(paras['vbar'][0],paras['vbar'][1])\n",
    "    kappa = np.random.uniform(paras['kappa'][0],paras['kappa'][1])\n",
    "    xi    = np.random.uniform(paras['xi'][0],paras['xi'][1])\n",
    "    rho   = np.random.uniform(paras['rho'][0],paras['rho'][1])\n",
    "    \n",
    "    model = HestonModel(rf, kappa, vbar, xi, rho)\n",
    "    sigmaIV = np.empty_like(M)\n",
    "    \n",
    "    \n",
    "    M   = np.sort(np.random.uniform(paras['M'][0], paras['M'][1], recover_y_dim[1]))\n",
    "    tau = np.sort(np.random.uniform(paras['tau'][0], paras['tau'][1], recover_y_dim[0]))\n",
    "    M , tau = np.meshgrid(M, tau)\n",
    "    \n",
    "    \n",
    "    for i in range(M.shape[0]) :\n",
    "        for j in range(M.shape[1]) :\n",
    "            price = model.PriceEuropean(S0, v0, tau[i,j], M[i,j])\n",
    "            try :\n",
    "                sigmaIV[i,j] = model.getImpliedVol(price, S0, tau[i,j], M[i,j])\n",
    "            except :\n",
    "                failed_sample_num += 1 \n",
    "                flag = True\n",
    "                break\n",
    "        if flag :\n",
    "            break\n",
    "       \n",
    "    if flag :\n",
    "        flag = False\n",
    "        failed_sample_num += 1\n",
    "        continue\n",
    "    \n",
    "    else :\n",
    "        x.append(np.concatenate((np.array([rf, v0,  vbar, kappa, xi, rho]),M.flatten(),tau.flatten()) ))\n",
    "        y.append(sigmaIV.flatten())\n",
    "        \n",
    "        if k%100 == 0:\n",
    "            print('Sampling Process finished : %d/%d'%(k, Nsample))\n",
    "        k += 1\n",
    "\n",
    "print(failed_sample_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformX(x):\n",
    "    xx = np.array(x.copy())\n",
    "    for i in range(len(para_index)):\n",
    "        lb , ub = paras[para_index[i]]\n",
    "        if i <= 5 :\n",
    "            xx[:,i] = ( 2*xx[:,i] - (ub+lb) ) / (ub-lb)\n",
    "        elif i == 6 :\n",
    "            xx[:,6:94] = ( 2*xx[:,6:94] - (ub+lb) ) / (ub-lb)\n",
    "        else :\n",
    "            xx[:,94:] = ( 2*xx[:,94:] - (ub+lb) ) / (ub-lb)\n",
    "    return xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To tensor and construct dataloader\n",
    "x , y = transformX(x) , np.array(y)\n",
    "x_tensor = torch.tensor(x, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "dataset = TensorDataset(x_tensor, y_tensor)\n",
    "\n",
    "# Dataset sizes\n",
    "total_size = len(dataset)\n",
    "train_size = int(total_size * 0.6)\n",
    "validate_size = int(total_size * 0.2)\n",
    "test_size  = total_size - train_size - validate_size\n",
    "\n",
    "# Construct datasets\n",
    "train, validate, test = random_split(dataset, [train_size, validate_size, test_size])\n",
    "\n",
    "# Save data\n",
    "data_saved = {\n",
    "    'paras'            : paras,          # paramter settings and range\n",
    "    'para_index'       : para_index,     # input order           \n",
    "    'Xnormalized_data'  : dataset,        # dataset.tensors gives (x_tensor, y_tensor) \n",
    "    'train'            : train,          # train[:] gives (x_tensor, y_tensor) in train\n",
    "    'validate'         : validate,\n",
    "    'test'             : test,     \n",
    "    'recover_y_dim'    : recover_y_dim\n",
    "}\n",
    "\n",
    "# Savepickle(data_saved, doc_path='./data/European/conv_training_data.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
