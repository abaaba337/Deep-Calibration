{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "import torch\n",
    "import numpy as np\n",
    "from src.Heston import HestonModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "def Savepickle(obj, doc_path):\n",
    "  with open(doc_path, 'wb') as file:\n",
    "         dill.dump(obj, file)     \n",
    "\n",
    "def Readpickle(doc_path):\n",
    "    with open(doc_path, 'rb') as file:\n",
    "        return dill.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# European"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Point-wise Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Settings\n",
    "paras = {\n",
    "                'rf'    : (0, 0.06),\n",
    "                'v0'    : (1e-3, 0.15),\n",
    "                'vbar'  : (1e-3, 0.10),\n",
    "                'kappa' : (1e-3, 5.0),\n",
    "                'xi'    : (1e-3, 1.0),\n",
    "                'rho'   : (-1.0, 0.0),\n",
    "                'M'     : (0.50, 1.5),\n",
    "                'tau'   : (0.25, 2.0)\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling Process finished : 10000/880000\n",
      "Sampling Process finished : 20000/880000\n",
      "Sampling Process finished : 30000/880000\n",
      "Sampling Process finished : 40000/880000\n",
      "Sampling Process finished : 50000/880000\n",
      "Sampling Process finished : 60000/880000\n",
      "Sampling Process finished : 70000/880000\n",
      "Sampling Process finished : 80000/880000\n",
      "Sampling Process finished : 90000/880000\n",
      "Sampling Process finished : 100000/880000\n",
      "Sampling Process finished : 110000/880000\n",
      "Sampling Process finished : 120000/880000\n",
      "Sampling Process finished : 130000/880000\n",
      "Sampling Process finished : 140000/880000\n",
      "Sampling Process finished : 150000/880000\n",
      "Sampling Process finished : 160000/880000\n",
      "Sampling Process finished : 170000/880000\n",
      "Sampling Process finished : 180000/880000\n",
      "Sampling Process finished : 190000/880000\n",
      "Sampling Process finished : 200000/880000\n",
      "Sampling Process finished : 210000/880000\n",
      "Sampling Process finished : 220000/880000\n",
      "Sampling Process finished : 230000/880000\n",
      "Sampling Process finished : 240000/880000\n",
      "Sampling Process finished : 250000/880000\n",
      "Sampling Process finished : 260000/880000\n",
      "Sampling Process finished : 270000/880000\n",
      "Sampling Process finished : 280000/880000\n",
      "Sampling Process finished : 290000/880000\n",
      "Sampling Process finished : 300000/880000\n",
      "Sampling Process finished : 310000/880000\n",
      "Sampling Process finished : 320000/880000\n",
      "Sampling Process finished : 330000/880000\n",
      "Sampling Process finished : 340000/880000\n",
      "Sampling Process finished : 350000/880000\n",
      "Sampling Process finished : 360000/880000\n",
      "Sampling Process finished : 370000/880000\n",
      "Sampling Process finished : 380000/880000\n",
      "Sampling Process finished : 390000/880000\n",
      "Sampling Process finished : 400000/880000\n",
      "Sampling Process finished : 410000/880000\n",
      "Sampling Process finished : 420000/880000\n",
      "Sampling Process finished : 430000/880000\n",
      "Sampling Process finished : 440000/880000\n",
      "Sampling Process finished : 450000/880000\n",
      "Sampling Process finished : 460000/880000\n",
      "Sampling Process finished : 470000/880000\n",
      "Sampling Process finished : 480000/880000\n",
      "Sampling Process finished : 490000/880000\n",
      "Sampling Process finished : 500000/880000\n",
      "Sampling Process finished : 510000/880000\n",
      "Sampling Process finished : 520000/880000\n",
      "Sampling Process finished : 530000/880000\n",
      "Sampling Process finished : 540000/880000\n",
      "Sampling Process finished : 550000/880000\n",
      "Sampling Process finished : 560000/880000\n",
      "Sampling Process finished : 570000/880000\n",
      "Sampling Process finished : 580000/880000\n",
      "Sampling Process finished : 590000/880000\n",
      "Sampling Process finished : 600000/880000\n",
      "Sampling Process finished : 610000/880000\n",
      "Sampling Process finished : 620000/880000\n",
      "Sampling Process finished : 630000/880000\n",
      "Sampling Process finished : 640000/880000\n",
      "Sampling Process finished : 650000/880000\n",
      "Sampling Process finished : 660000/880000\n",
      "Sampling Process finished : 670000/880000\n",
      "Sampling Process finished : 680000/880000\n",
      "Sampling Process finished : 690000/880000\n",
      "Sampling Process finished : 700000/880000\n",
      "Sampling Process finished : 710000/880000\n",
      "Sampling Process finished : 720000/880000\n",
      "Sampling Process finished : 730000/880000\n",
      "Sampling Process finished : 740000/880000\n",
      "Sampling Process finished : 750000/880000\n",
      "Sampling Process finished : 760000/880000\n",
      "Sampling Process finished : 770000/880000\n",
      "Sampling Process finished : 780000/880000\n",
      "Sampling Process finished : 790000/880000\n",
      "Sampling Process finished : 800000/880000\n",
      "Sampling Process finished : 810000/880000\n",
      "Sampling Process finished : 820000/880000\n",
      "Sampling Process finished : 830000/880000\n",
      "Sampling Process finished : 840000/880000\n",
      "Sampling Process finished : 850000/880000\n",
      "Sampling Process finished : 860000/880000\n",
      "Sampling Process finished : 870000/880000\n",
      "Sampling Process finished : 880000/880000\n",
      "41694\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic data\n",
    "x = [] # [ [rf, v0,  vbar, kappa, xi, rho, M, tau] ]\n",
    "y = []  # [ sigmaIV ]\n",
    "\n",
    "i , Nsample , failed_sample_num = 1 , 880000 , 0\n",
    "\n",
    "while i <= Nsample:\n",
    "    S0    = 1\n",
    "    rf    = np.random.uniform(paras['rf'][0],paras['rf'][1])\n",
    "    v0    = np.random.uniform(paras['v0'][0],paras['v0'][1])\n",
    "    vbar  = np.random.uniform(paras['vbar'][0],paras['vbar'][1])\n",
    "    kappa = np.random.uniform(paras['kappa'][0],paras['kappa'][1])\n",
    "    xi    = np.random.uniform(paras['xi'][0],paras['xi'][1])\n",
    "    rho   = np.random.uniform(paras['rho'][0],paras['rho'][1])\n",
    "    M     = np.random.uniform(paras['M'][0],paras['M'][1])\n",
    "    tau   = np.random.uniform(paras['tau'][0],paras['tau'][1])\n",
    "    \n",
    "    try :\n",
    "        model = HestonModel(rf, kappa, vbar, xi, rho)\n",
    "        price = model.PriceEuropean(S0, v0, tau, K=M*S0)\n",
    "        sigmaIV = model.getImpliedVol(price, S0, tau, K=M*S0)\n",
    "        \n",
    "        if sigmaIV == -1 :\n",
    "            failed_sample_num += 1 \n",
    "            continue\n",
    "\n",
    "        else :\n",
    "            \n",
    "            x.append([rf, v0,  vbar, kappa, xi, rho, M, tau])\n",
    "            y.append(sigmaIV)\n",
    "            \n",
    "            if i%10000 == 0:\n",
    "                print('Sampling Process finished : %d/%d'%(i, Nsample))   \n",
    "            i += 1\n",
    "        \n",
    "    except :\n",
    "        failed_sample_num += 1 \n",
    "        continue\n",
    "    \n",
    "print(failed_sample_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized X := (2*X-Sigma)*Lambda matrix multiplication\n",
    "# torch.matmul((2 * dataset.tensors[0] - Sigma), Lambda)\n",
    "columns  = [(bound[1] + bound[0]) for bound in paras.values()]\n",
    "diagnals = [1/(bound[1] - bound[0]) for bound in paras.values()]\n",
    "\n",
    "Sigma  = torch.tensor(columns)\n",
    "Lambda = torch.diag(torch.tensor(diagnals))\n",
    "\n",
    "# To tensor and construct dataloader\n",
    "x , y = np.array(x) , np.array(y).reshape(-1, 1)\n",
    "x_tensor = torch.tensor(x, dtype=torch.float32)\n",
    "x_tensor = torch.matmul((2 * x_tensor - Sigma), Lambda)\n",
    "\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "dataset = TensorDataset(x_tensor, y_tensor)\n",
    "\n",
    "# Dataset sizes\n",
    "total_size = len(dataset)\n",
    "train_size = int(total_size * 0.6)\n",
    "validate_size = int(total_size * 0.2)\n",
    "test_size  = total_size - train_size - validate_size\n",
    "\n",
    "# Construct datasets\n",
    "train, validate, test = random_split(dataset, [train_size, validate_size, test_size])\n",
    "\n",
    "# Save data\n",
    "data_saved = {\n",
    "    'paras'            : paras,          # paramter settings and range\n",
    "    'dataset'          : dataset,        # dataset.tensors gives (x_tensor, y_tensor) \n",
    "    'train'            : train,          # train[:] gives (x_tensor, y_tensor) in train\n",
    "    'validate'         : validate,\n",
    "    'test'             : test,\n",
    "    'x_norm_mat'       : (Sigma, Lambda)\n",
    "}\n",
    "\n",
    "# Savepickle(data_saved, doc_path='./data/European/point_wise_training_data88w.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid-base Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Settings\n",
    "paras = {\n",
    "                'rf'    : (0, 0.06),\n",
    "                'v0'    : (1e-3, 0.15),\n",
    "                'vbar'  : (1e-3, 0.10),\n",
    "                'kappa' : (1e-3, 5.0),\n",
    "                'xi'    : (1e-3, 1.0),\n",
    "                'rho'   : (-1.0, 0.0),\n",
    "                'M'     : np.arange(0.5,1.6,0.1),\n",
    "                'tau'   : np.arange(0.25,2.25,0.25)\n",
    "            }\n",
    "\n",
    "recover_y_dim = (len(paras['tau']) , len(paras['M']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling Process finished : 1000/10000\n",
      "Sampling Process finished : 2000/10000\n",
      "Sampling Process finished : 3000/10000\n",
      "Sampling Process finished : 4000/10000\n",
      "Sampling Process finished : 5000/10000\n",
      "Sampling Process finished : 6000/10000\n",
      "Sampling Process finished : 7000/10000\n",
      "Sampling Process finished : 8000/10000\n",
      "Sampling Process finished : 9000/10000\n",
      "Sampling Process finished : 10000/10000\n",
      "9958\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic data\n",
    "x  = [] # [ [rf, v0,  vbar, kappa, xi, rho] ]\n",
    "y  = [] # [ sigmaIV ]\n",
    "\n",
    "k , Nsample , failed_sample_num , flag = 1 , 10000 , 0 , False #17000\n",
    "M , tau = paras['M'] , paras['tau']\n",
    "M, tau = np.meshgrid(M, tau)\n",
    "\n",
    "\n",
    "while k <= Nsample:\n",
    "    rf    = np.random.uniform(paras['rf'][0],paras['rf'][1])\n",
    "    S0    = 1.0\n",
    "    v0    = np.random.uniform(paras['v0'][0],paras['v0'][1])\n",
    "    vbar  = np.random.uniform(paras['vbar'][0],paras['vbar'][1])\n",
    "    kappa = np.random.uniform(paras['kappa'][0],paras['kappa'][1])\n",
    "    xi    = np.random.uniform(paras['xi'][0],paras['xi'][1])\n",
    "    rho   = np.random.uniform(paras['rho'][0],paras['rho'][1])\n",
    "    \n",
    "    model = HestonModel(rf, kappa, vbar, xi, rho)\n",
    "    sigmaIV = np.empty_like(M)\n",
    "    \n",
    "    for i in range(M.shape[0]) :\n",
    "        for j in range(M.shape[1]) :\n",
    "            price = model.PriceEuropean(S0, v0, tau[i,j], M[i,j])\n",
    "            try :\n",
    "                sigmaIV[i,j] = model.getImpliedVol(price, S0, tau[i,j], M[i,j])\n",
    "            except :\n",
    "                failed_sample_num += 1 \n",
    "                flag = True\n",
    "                break\n",
    "        if flag :\n",
    "            break\n",
    "       \n",
    "    if flag :\n",
    "        flag = False\n",
    "        failed_sample_num += 1\n",
    "        continue\n",
    "    \n",
    "    else :\n",
    "        x.append([rf, v0,  vbar, kappa, xi, rho])\n",
    "        y.append(sigmaIV.flatten())\n",
    "        \n",
    "        if k%1000 == 0:\n",
    "            print('Sampling Process finished : %d/%d'%(k, Nsample))\n",
    "        k += 1\n",
    "\n",
    "print(failed_sample_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized X := (2*X-Sigma)*Lambda matrix multiplication\n",
    "# torch.matmul((2 * dataset.tensors[0] - Sigma), Lambda)\n",
    "columns  = [(bound[1] + bound[0]) for bound in paras.values() if len(bound) == 2]\n",
    "diagnals = [1/(bound[1] - bound[0]) for bound in paras.values() if len(bound) == 2]\n",
    "\n",
    "Sigma  = torch.tensor(columns)\n",
    "Lambda = torch.diag(torch.tensor(diagnals))\n",
    "\n",
    "# To tensor and construct dataloader\n",
    "x , y = np.array(x) , np.array(y)\n",
    "x_tensor = torch.tensor(x, dtype=torch.float32)\n",
    "x_tensor = torch.matmul((2 * x_tensor - Sigma), Lambda)\n",
    "\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "dataset = TensorDataset(x_tensor, y_tensor)\n",
    "\n",
    "# Dataset sizes\n",
    "total_size = len(dataset)\n",
    "train_size = int(total_size * 0.6)\n",
    "validate_size = int(total_size * 0.2)\n",
    "test_size  = total_size - train_size - validate_size\n",
    "\n",
    "# Construct datasets\n",
    "train, validate, test = random_split(dataset, [train_size, validate_size, test_size])\n",
    "\n",
    "# Save data\n",
    "data_saved = {\n",
    "    'paras'            : paras,          # paramter settings and range\n",
    "    'dataset'          : dataset,        # dataset.tensors gives (x_tensor, y_tensor) \n",
    "    'train'            : train,          # train[:] gives (x_tensor, y_tensor) in train\n",
    "    'validate'         : validate,\n",
    "    'test'             : test,\n",
    "    'x_norm_mat'       : (Sigma, Lambda),\n",
    "    'recover_y_dim'    : recover_y_dim\n",
    "}\n",
    "\n",
    "# Savepickle(data_saved, doc_path='./data/European/grid_based_training_data1w.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter Settings\n",
    "paras = {\n",
    "                'rf'    : (0, 0.06),\n",
    "                'v0'    : (1e-3, 0.15),\n",
    "                'vbar'  : (1e-3, 0.10),\n",
    "                'kappa' : (1e-3, 5.0),\n",
    "                'xi'    : (1e-3, 1.0),\n",
    "                'rho'   : (-1.0, 0.0),\n",
    "                'M'     : np.arange(0.5,1.6,0.1),\n",
    "                'tau'   : np.arange(0.25,2.25,0.25)\n",
    "            }\n",
    "\n",
    "recover_y_dim = (len(paras['tau']) , len(paras['M']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling Process finished : 1000/5000\n",
      "Sampling Process finished : 2000/5000\n",
      "Sampling Process finished : 3000/5000\n",
      "Sampling Process finished : 4000/5000\n",
      "Sampling Process finished : 5000/5000\n",
      "4888\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic data\n",
    "x  = [] # [ [rf, v0,  vbar, kappa, xi, rho] ]\n",
    "y  = [] # [ sigmaIV ]\n",
    "\n",
    "k , Nsample , failed_sample_num , flag = 1 , 5000 , 0 , False #17000\n",
    "M , tau = paras['M'] , paras['tau']\n",
    "M, tau = np.meshgrid(M, tau)\n",
    "\n",
    "\n",
    "while k <= Nsample:\n",
    "    rf    = 0.025\n",
    "    S0    = 1.0\n",
    "    v0    = np.random.uniform(paras['v0'][0],paras['v0'][1])\n",
    "    vbar  = np.random.uniform(paras['vbar'][0],paras['vbar'][1])\n",
    "    kappa = np.random.uniform(paras['kappa'][0],paras['kappa'][1])\n",
    "    xi    = np.random.uniform(paras['xi'][0],paras['xi'][1])\n",
    "    rho   = np.random.uniform(paras['rho'][0],paras['rho'][1])\n",
    "    \n",
    "    model = HestonModel(rf, kappa, vbar, xi, rho)\n",
    "    sigmaIV = np.empty_like(M)\n",
    "    \n",
    "    for i in range(M.shape[0]) :\n",
    "        for j in range(M.shape[1]) :\n",
    "            price = model.PriceEuropean(S0, v0, tau[i,j], M[i,j])\n",
    "            try :\n",
    "                sigmaIV[i,j] = model.getImpliedVol(price, S0, tau[i,j], M[i,j])\n",
    "            except :\n",
    "                failed_sample_num += 1 \n",
    "                flag = True\n",
    "                break\n",
    "        if flag :\n",
    "            break\n",
    "       \n",
    "    if flag :\n",
    "        flag = False\n",
    "        failed_sample_num += 1\n",
    "        continue\n",
    "    \n",
    "    else :\n",
    "        x.append([rf, v0,  vbar, kappa, xi, rho])\n",
    "        y.append(sigmaIV.flatten())\n",
    "        \n",
    "        if k%1000 == 0:\n",
    "            print('Sampling Process finished : %d/%d'%(k, Nsample))\n",
    "        k += 1\n",
    "\n",
    "print(failed_sample_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalized X := (2*X-Sigma)*Lambda matrix multiplication\n",
    "# torch.matmul((2 * dataset.tensors[0] - Sigma), Lambda)\n",
    "columns  = [(bound[1] + bound[0]) for bound in paras.values() if len(bound) == 2]\n",
    "diagnals = [1/(bound[1] - bound[0]) for bound in paras.values() if len(bound) == 2]\n",
    "\n",
    "Sigma  = torch.tensor(columns)\n",
    "Lambda = torch.diag(torch.tensor(diagnals))\n",
    "\n",
    "# To tensor and construct dataloader\n",
    "x , y = np.array(x) , np.array(y)\n",
    "x_tensor = torch.tensor(x, dtype=torch.float32)\n",
    "x_tensor = torch.matmul((2 * x_tensor - Sigma), Lambda)\n",
    "\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "dataset = TensorDataset(x_tensor, y_tensor)\n",
    "\n",
    "\n",
    "# Save data\n",
    "data_saved = {\n",
    "    'paras'            : paras,          # paramter settings and range\n",
    "    'dataset'          : dataset,        # dataset.tensors gives (x_tensor, y_tensor) \n",
    "    'x_norm_mat'       : (Sigma, Lambda),\n",
    "    'recover_y_dim'    : recover_y_dim\n",
    "}\n",
    "\n",
    "# Savepickle(data_saved, doc_path='./data/European/calibration_data_new.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
